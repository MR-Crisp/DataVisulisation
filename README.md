# ğŸ“Š Data Visualisation Project â€” Group D
**Tama Â· Omar Â· Aamir Â· Khalood**

---

## ğŸ“‹ Overview

This project explores unsupervised machine learning and dimensionality reduction techniques to produce meaningful, interactive data visualisations. The system clusters high-dimensional data and renders it in low-dimensional space, enabling pattern discovery and exploratory analysis without labelled data.

---

## ğŸ¯ Aims & Objectives

- Clearly define the problem and its significance
- Apply dimensionality reduction to preserve meaningful data properties while reducing computational cost
- Implement and evaluate unsupervised clustering algorithms
- Produce intuitive, high-quality visual outputs from which conclusions can be drawn

---

## ğŸ§  AI Techniques

### Dimensionality Reduction
We apply at least one of the following techniques to reduce feature space while retaining core data properties:
- **PCA** (Principal Component Analysis)
- **VAE** (Variational Autoencoder)
- **FastICA** (Independent Component Analysis) â€” under consideration

### Clustering Algorithms
We researched and evaluated four unsupervised learning algorithms:

| Algorithm | Key Strength | Key Weakness |
|-----------|-------------|--------------|
| **K-Means** | Fast, scalable, interpretable | Assumes spherical clusters; sensitive to outliers |
| **SOM** (Self-Organising Maps) | Natural 2D output; topology-preserving | Slow; many hyperparameters |
| **DBSCAN** | No need to specify K; handles arbitrary shapes | Sensitive to Îµ and minPts; struggles with varying densities |
| **GMM** (Gaussian Mixture Models) | Soft clustering; flexible elliptical shapes | Assumes Gaussian distribution; slower than K-Means |

---

## âš™ï¸ System Design

### Functional Requirements
- Data ingestion and cleaning pipeline
- Dimensionality reduction module
- Clustering module with autonomous K selection (e.g. elbow method / BIC)
- Interactive visualisation layer

### Non-Functional Requirements
- **Platform Independence:** Runs on Windows, macOS, and Linux
- **Resource Constraints:** Functions on standard hardware (8GB RAM, 4-core CPU)
- **Modular Architecture:** Clear separation between data pipeline, AI models, and visualisation layers
- **Learnability:** Users can understand basic interactions without prior instructions

---

## ğŸ“ Evaluation Metrics

### Quantitative
- **Information Entropy** â€” Measures how much of the original data's "usefulness" is preserved in the output. Calculated as `information(x) = -log(p(x))`, scored 0â€“1.0.
- **Silhouette Coefficient** â€” Validates clustering effectiveness. Score of +1 = well-clustered; -1 = misclassified. Formula: `S = (b - a) / max(a, b)`

### Qualitative
Each output is rated 1â€“10 across four dimensions by multiple evaluators (peers, lecturers, supervisors):

| Dimension | Questions |
|-----------|-----------|
| **Relevance** | Does the output address the goal? Are there irrelevant points? |
| **Accuracy** | Does it accurately reflect the input data? Any hallucinations? |
| **Coherence** | Is it easy to understand? Is there unnecessary data? |
| **Usefulness** | Can clear conclusions be drawn? Are patterns/biases visible? |

**Scale:** 1â€“3 (poor) Â· 4â€“6 (adequate) Â· 6â€“8 (good) Â· 9â€“10 (excellent)

---

## ğŸ—‚ï¸ Project Structure

```
â”œâ”€â”€ data/               # Raw and processed datasets
â”œâ”€â”€ pipeline/           # Data cleaning and ingestion scripts
â”œâ”€â”€ models/             # Dimensionality reduction & clustering modules
â”œâ”€â”€ visualisation/      # UI and visual output layer
â”œâ”€â”€ evaluation/         # Metrics and evaluation scripts
â””â”€â”€ docs/               # Report and documentation
```

---

## ğŸš€ Getting Started

> Setup instructions will be added as the project develops.

```bash
# Clone the repository
git clone <repo-url>
cd <repo-name>

# Install dependencies
pip install -r requirements.txt

# Run the application
python main.py
```

---

## ğŸ“… Progress & Reports

Weekly progress reports and retrospective analysis are tracked in the appendices of the project report. Refer to the repository's commit history for a timeline of development activity.

> âš ï¸ A consistent commit history is maintained throughout the semester in line with project requirements.

---

## ğŸ¤– Generative AI Usage

Details of any generative AI tools used during this project are documented in Appendix 3 of the full report.

---

